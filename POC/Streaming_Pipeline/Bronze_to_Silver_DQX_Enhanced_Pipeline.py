# Databricks notebook source
# MAGIC %md
# MAGIC # Bronze to Silver Streaming Pipeline with DQX Quality Framework
# MAGIC 
# MAGIC **Weather Data Processing Pipeline with Databricks DQX Quality Checks**
# MAGIC 
# MAGIC This notebook extends the Bronze_to_Silver_Pipeline with:
# MAGIC - **Databricks DQX Framework** integration for data quality validation
# MAGIC - Comprehensive quality rule checks (type validation, range checks, format validation)
# MAGIC - Quality metadata fields (flag_check, description_failure, DQX lineage) **added to same Silver table**
# MAGIC - **Single Enhanced Silver Table** - ALL records saved with quality status flags
# MAGIC - Real-time streaming with quality monitoring
# MAGIC 
# MAGIC **Enhanced Architecture:**
# MAGIC ```
# MAGIC Bronze Layer ‚Üí JSON Parsing ‚Üí DQX Quality Checks ‚Üí Single Enhanced Silver Table (ALL records with quality flags)
# MAGIC ```

# COMMAND ----------

# MAGIC %md
# MAGIC ## 1. Configuration Widgets - Extended from Bronze_to_Silver_Pipeline

# COMMAND ----------

# Create configuration widgets matching the Bronze_to_Silver_Pipeline configuration
# Common properties aligned with EventHub_Listener_HiveMetastore_Databricks
dbutils.widgets.text("eventhub_scope", "rxr-idi-adb-secret-scope", "Secret Scope Name")
dbutils.widgets.text("storage_account", "idisitcusadls2", "Storage Account Name")
dbutils.widgets.text("container", "eventhub-test", "ADLS Container Name")

# Source configuration - matching Bronze layer
dbutils.widgets.text("bronze_database", "bronze", "Bronze Database Name")
dbutils.widgets.text("bronze_table", "weather_events_raw", "Bronze Table Name")

# Target configuration - Single enhanced Silver layer with DQX quality metadata
dbutils.widgets.text("silver_database", "bronze", "Silver Database Name")
dbutils.widgets.text("silver_table", "weather_events_silver", "Silver Table Name")

# Pipeline paths
dbutils.widgets.text("silver_path", "/mnt/silver/weather", "Silver Layer Path")
dbutils.widgets.text("checkpoint_path", "/mnt/checkpoints/bronze-to-silver-dqx", "DQX Pipeline Checkpoint Path")

# DQX Quality configuration
dbutils.widgets.dropdown("quality_criticality", "error", ["error", "warn"], "DQX Quality Criticality Level")
dbutils.widgets.text("quality_threshold", "0.95", "Quality Pass Threshold (0-1)")

# Streaming configuration - matching Bronze_to_Silver_Pipeline values
dbutils.widgets.dropdown("trigger_mode", "5 seconds", ["1 second", "5 seconds", "10 seconds", "1 minute"], "Trigger Interval")
dbutils.widgets.text("max_events_per_trigger", "10000", "Max Events Per Trigger")

# Get configuration values
SECRET_SCOPE = dbutils.widgets.get("eventhub_scope")
STORAGE_ACCOUNT = dbutils.widgets.get("storage_account")
CONTAINER = dbutils.widgets.get("container")

BRONZE_DATABASE = dbutils.widgets.get("bronze_database")
BRONZE_TABLE = dbutils.widgets.get("bronze_table")
SILVER_DATABASE = dbutils.widgets.get("silver_database")
SILVER_TABLE = dbutils.widgets.get("silver_table")

BRONZE_FULL_TABLE = f"{BRONZE_DATABASE}.{BRONZE_TABLE}"
SILVER_FULL_TABLE = f"{SILVER_DATABASE}.{SILVER_TABLE}"

SILVER_PATH = dbutils.widgets.get("silver_path")
CHECKPOINT_PATH = dbutils.widgets.get("checkpoint_path")

QUALITY_CRITICALITY = dbutils.widgets.get("quality_criticality")
QUALITY_THRESHOLD = float(dbutils.widgets.get("quality_threshold"))
TRIGGER_MODE = dbutils.widgets.get("trigger_mode")
MAX_EVENTS_PER_TRIGGER = int(dbutils.widgets.get("max_events_per_trigger"))

print("üîß Bronze to Silver DQX Enhanced Pipeline Configuration:")
print(f"   Secret Scope: {SECRET_SCOPE}")
print(f"   Storage Account: {STORAGE_ACCOUNT}")
print(f"   Container: {CONTAINER}")
print(f"   Source: {BRONZE_FULL_TABLE}")
print(f"   Enhanced Silver Target: {SILVER_FULL_TABLE} (Single table with DQX quality flags)")
print(f"   DQX Quality Criticality: {QUALITY_CRITICALITY}")
print(f"   Quality Threshold: {QUALITY_THRESHOLD}")
print(f"   Trigger: {TRIGGER_MODE}")
print(f"   Max Events/Trigger: {MAX_EVENTS_PER_TRIGGER}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 2. Import Libraries and Initialize Databricks DQX Framework

# COMMAND ----------

import json
import logging
from datetime import datetime, timezone
from typing import Dict, Any, Optional, List, Tuple
import uuid

from pyspark.sql import SparkSession, DataFrame
from pyspark.sql.functions import *
from pyspark.sql.types import *
from pyspark.sql.streaming import StreamingQuery
from delta.tables import DeltaTable

# Initialize DQX Framework with proper error handling
DQX_AVAILABLE = False
DQEngine = None
DQRowRule = None
DQDatasetRule = None
check_funcs = None

print("üîç Initializing Databricks DQX Framework...")

try:
    # Import DQX components with correct API
    from databricks.labs.dqx.engine import DQEngine
    from databricks.labs.dqx.rule import DQRowRule, DQDatasetRule, DQForEachColRule
    from databricks.labs.dqx import check_funcs
    from databricks.sdk import WorkspaceClient
    
    # Initialize WorkspaceClient for DQX
    try:
        ws = WorkspaceClient()
        print("‚úÖ Databricks WorkspaceClient initialized successfully")
    except Exception as ws_error:
        print(f"‚ö†Ô∏è WorkspaceClient initialization warning: {ws_error}")
        ws = None
    
    # Initialize DQEngine
    dq_engine = DQEngine(ws) if ws else DQEngine()
    
    print("‚úÖ Databricks DQX Framework imported and initialized successfully")
    DQX_AVAILABLE = True
    
except ImportError as import_error:
    print(f"‚ö†Ô∏è DQX Framework import failed: {import_error}")
    
    try:
        # Try installing DQX framework
        print("üì¶ Installing databricks-labs-dqx...")
        import subprocess
        import sys
        
        result = subprocess.run([sys.executable, "-m", "pip", "install", "databricks-labs-dqx"], 
                              capture_output=True, text=True, timeout=180)
        
        if result.returncode == 0:
            print("‚úÖ DQX Framework installation completed")
            
            # Import after installation
            from databricks.labs.dqx.engine import DQEngine
            from databricks.labs.dqx.rule import DQRowRule, DQDatasetRule, DQForEachColRule
            from databricks.labs.dqx import check_funcs
            from databricks.sdk import WorkspaceClient
            
            # Initialize components
            try:
                ws = WorkspaceClient()
                dq_engine = DQEngine(ws)
            except:
                dq_engine = DQEngine()
                
            print("‚úÖ Databricks DQX Framework ready after installation")
            DQX_AVAILABLE = True
            
        else:
            raise ImportError("DQX installation failed")
            
    except Exception as install_error:
        print(f"‚ùå Failed to install/initialize DQX Framework: {install_error}")
        print("üîÑ Falling back to basic data quality validation...")
        DQX_AVAILABLE = False

# Configure Spark for optimal streaming performance with DQX
spark.conf.set("spark.sql.streaming.metricsEnabled", "true")
spark.conf.set("spark.sql.adaptive.enabled", "true")
spark.conf.set("spark.sql.adaptive.coalescePartitions.enabled", "true")
spark.conf.set("spark.databricks.delta.autoOptimize.optimizeWrite", "true")
spark.conf.set("spark.databricks.delta.autoOptimize.autoCompact", "true")

# Set checkpoint configuration for reliability (30 seconds interval)
spark.conf.set("spark.sql.streaming.checkpointLocation.compression.codec", "lz4")

print("‚úÖ Spark configured for DQX-enhanced streaming pipeline")
print(f"   DQX Framework Available: {'‚úÖ Yes' if DQX_AVAILABLE else '‚ùå No - Using Fallback'}")
print(f"   Checkpoint interval: 30 seconds")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 3. Enhanced Silver Layer Schema with DQX Quality Metadata (Single Table)

# COMMAND ----------

# Enhanced Silver layer schema: ALL Bronze_to_Silver_Pipeline fields + DQX quality metadata fields
# This extends the original Silver schema with quality flags - ALL records go to same table
silver_enhanced_schema = StructType([
    # Business Weather Data (same as Bronze_to_Silver_Pipeline)
    StructField("event_id", StringType(), False),  # Business event identifier
    StructField("city", StringType(), True),
    StructField("latitude", DoubleType(), True),
    StructField("longitude", DoubleType(), True),
    StructField("temperature", DoubleType(), True),
    StructField("humidity", DoubleType(), True),
    StructField("wind_speed", DoubleType(), True),
    StructField("pressure", DoubleType(), True),
    StructField("precipitation", DoubleType(), True),
    StructField("cloud_cover", DoubleType(), True),
    StructField("weather_condition", StringType(), True),
    StructField("weather_timestamp", TimestampType(), True),  # From payload
    StructField("data_source", StringType(), True),
    StructField("weather_cluster_id", StringType(), True),  # From payload
    StructField("notebook_path", StringType(), True),
    
    # Inherited Technical Fields from Bronze (same as Bronze_to_Silver_Pipeline)
    StructField("source_record_id", StringType(), False),  # Original bronze record_id
    StructField("bronze_ingestion_timestamp", TimestampType(), False),
    StructField("bronze_ingestion_date", DateType(), False),
    StructField("bronze_ingestion_hour", IntegerType(), False),
    
    # Event Hub Technical Fields (inherited - same as Bronze_to_Silver_Pipeline)
    StructField("eventhub_partition", StringType(), True),
    StructField("eventhub_offset", StringType(), True),
    StructField("eventhub_sequence_number", LongType(), True),
    StructField("eventhub_enqueued_time", TimestampType(), True),
    StructField("eventhub_partition_key", StringType(), True),
    
    # Processing Context (inherited - same as Bronze_to_Silver_Pipeline)
    StructField("processing_cluster_id", StringType(), True),
    StructField("consumer_group", StringType(), True),
    StructField("eventhub_name", StringType(), True),
    
    # Silver Processing Metadata (same as Bronze_to_Silver_Pipeline)
    StructField("silver_processing_timestamp", TimestampType(), False),
    StructField("silver_processing_date", DateType(), False),  # Partition field
    StructField("silver_processing_hour", IntegerType(), False),  # Partition field
    
    # Payload information (same as Bronze_to_Silver_Pipeline)
    StructField("original_payload_size_bytes", IntegerType(), True),
    StructField("parsing_status", StringType(), False),  # SUCCESS, PARTIAL, FAILED
    StructField("parsing_errors", ArrayType(StringType()), True),
    
    # **NEW: DQX Quality Metadata Fields - Added to SAME Silver Table**
    StructField("flag_check", StringType(), False),  # PASS, FAIL, WARNING
    StructField("description_failure", StringType(), True),  # Details about validation failures
    StructField("dqx_rule_results", ArrayType(StringType()), True),  # DQX rule execution results
    StructField("dqx_quality_score", DoubleType(), False),  # Overall quality score (0.0-1.0)
    StructField("dqx_validation_timestamp", TimestampType(), False),  # When DQX validation occurred
    StructField("dqx_lineage_id", StringType(), True),  # DQX lineage tracking identifier
    StructField("dqx_criticality_level", StringType(), True),  # error, warn
    StructField("failed_rules_count", IntegerType(), False),  # Number of failed DQX rules
    StructField("passed_rules_count", IntegerType(), False)   # Number of passed DQX rules
])

print("‚úÖ Enhanced Silver layer schema defined (Single Table Approach)")
print("   Base data: Same as Bronze_to_Silver_Pipeline")
print("   Enhanced: Added 9 DQX quality metadata fields")
print("   Approach: ALL records (valid + invalid) saved to same enhanced Silver table")
print("   Quality tracking: flag_check field indicates PASS/FAIL status")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 4. DQX Quality Rules Definition for Weather Data

# COMMAND ----------

class WeatherDataDQXRules:
    """Databricks DQX quality rules for weather data validation"""
    
    def __init__(self, criticality: str = "error"):
        self.criticality = criticality
        self.dqx_rules = []
        self._define_weather_quality_rules()
    
    def _define_weather_quality_rules(self):
        """Define comprehensive DQX quality rules for weather data"""
        
        if not DQX_AVAILABLE:
            print("‚ö†Ô∏è DQX Framework not available - rules will be empty")
            return
        
        try:
            print("üìã Defining DQX quality rules for weather data validation...")
            
            # 1. **Completeness Checks** - Critical fields must not be null/empty
            self.dqx_rules.extend([
                DQRowRule(
                    name="event_id_not_null",
                    criticality=self.criticality,
                    check_func=check_funcs.is_not_null_and_not_empty,
                    column="event_id"
                ),
                DQRowRule(
                    name="city_not_null",
                    criticality=self.criticality,
                    check_func=check_funcs.is_not_null_and_not_empty,
                    column="city"
                ),
                DQRowRule(
                    name="weather_timestamp_not_null",
                    criticality=self.criticality,
                    check_func=check_funcs.is_not_null,
                    column="weather_timestamp"
                )
            ])
            
            # 2. **Type Validation** - Numeric fields must be valid numbers
            self.dqx_rules.extend([
                DQRowRule(
                    name="temperature_is_numeric",
                    criticality="warn",
                    check_func=check_funcs.is_not_null,
                    column="temperature"
                ),
                DQRowRule(
                    name="humidity_is_numeric",
                    criticality="warn",
                    check_func=check_funcs.is_not_null,
                    column="humidity"
                ),
                DQRowRule(
                    name="latitude_is_numeric",
                    criticality="warn",
                    check_func=check_funcs.is_not_null,
                    column="latitude"
                ),
                DQRowRule(
                    name="longitude_is_numeric",
                    criticality="warn",
                    check_func=check_funcs.is_not_null,
                    column="longitude"
                )
            ])
            
            # 3. **Range Validation** - Weather values must be within realistic bounds
            try:
                self.dqx_rules.extend([
                    DQRowRule(
                        name="temperature_realistic_range",
                        criticality="warn",
                        check_func=check_funcs.sql_expression,
                        column="temperature",
                        check_func_kwargs={
                            "expression": "temperature >= -60 AND temperature <= 60",
                            "msg": "Temperature should be between -60¬∞C and 60¬∞C"
                        }
                    ),
                    DQRowRule(
                        name="humidity_valid_percentage",
                        criticality="warn", 
                        check_func=check_funcs.sql_expression,
                        column="humidity",
                        check_func_kwargs={
                            "expression": "humidity >= 0 AND humidity <= 100",
                            "msg": "Humidity should be between 0% and 100%"
                        }
                    ),
                    DQRowRule(
                        name="latitude_valid_range",
                        criticality="warn",
                        check_func=check_funcs.sql_expression,
                        column="latitude",
                        check_func_kwargs={
                            "expression": "latitude >= -90 AND latitude <= 90",
                            "msg": "Latitude should be between -90 and 90 degrees"
                        }
                    ),
                    DQRowRule(
                        name="longitude_valid_range",
                        criticality="warn",
                        check_func=check_funcs.sql_expression,
                        column="longitude",
                        check_func_kwargs={
                            "expression": "longitude >= -180 AND longitude <= 180",
                            "msg": "Longitude should be between -180 and 180 degrees"
                        }
                    )
                ])
            except Exception as range_error:
                print(f"‚ö†Ô∏è Range validation rules skipped: {range_error}")
            
            # 4. **Format Validation** - String fields should follow expected patterns
            try:
                self.dqx_rules.extend([
                    DQRowRule(
                        name="city_name_format",
                        criticality="warn",
                        check_func=check_funcs.sql_expression,
                        column="city",
                        check_func_kwargs={
                            "expression": "LENGTH(city) >= 2 AND LENGTH(city) <= 50",
                            "msg": "City name should be between 2 and 50 characters"
                        }
                    ),
                    DQRowRule(
                        name="data_source_valid",
                        criticality="warn",
                        check_func=check_funcs.sql_expression,
                        column="data_source",
                        check_func_kwargs={
                            "expression": "data_source IN ('databricks_weather_simulator', 'external_api', 'sensor_data')",
                            "msg": "Data source must be from approved sources"
                        }
                    )
                ])
            except Exception as format_error:
                print(f"‚ö†Ô∏è Format validation rules skipped: {format_error}")
            
            # 5. **Consistency Checks** - Cross-field validation
            try:
                self.dqx_rules.append(
                    DQRowRule(
                        name="timestamp_not_future",
                        criticality="error",
                        check_func=check_funcs.sql_expression,
                        column="weather_timestamp",
                        check_func_kwargs={
                            "expression": "weather_timestamp <= CURRENT_TIMESTAMP()",
                            "msg": "Weather timestamp cannot be in the future"
                        }
                    )
                )
            except Exception as consistency_error:
                print(f"‚ö†Ô∏è Consistency validation rules skipped: {consistency_error}")
            
            print(f"‚úÖ Defined {len(self.dqx_rules)} DQX quality rules for weather data")
            print("   - Completeness: event_id, city, timestamp validation")
            print("   - Type validation: Numeric fields validation")  
            print("   - Range validation: Temperature, humidity, coordinates")
            print("   - Format validation: City name, data source")
            print("   - Consistency: Timestamp logic validation")
            
        except Exception as e:
            print(f"‚ùå Error defining DQX quality rules: {e}")
            self.dqx_rules = []
    
    def get_rules(self) -> List:
        """Get all defined DQX rules"""
        return self.dqx_rules
    
    def add_custom_rule(self, rule):
        """Add custom DQX rule"""
        if DQX_AVAILABLE:
            self.dqx_rules.append(rule)
            print(f"‚úÖ Added custom DQX rule: {rule.name if hasattr(rule, 'name') else 'unnamed'}")

# Initialize DQX quality rules for weather data
weather_dqx_rules = WeatherDataDQXRules(QUALITY_CRITICALITY)
quality_rules = weather_dqx_rules.get_rules()

print(f"‚úÖ Weather Data DQX Quality Rules initialized: {len(quality_rules)} rules")
print(f"   Criticality Level: {QUALITY_CRITICALITY}")
print(f"   Quality Threshold: {QUALITY_THRESHOLD}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 5. Enhanced Weather Data Transformation with DQX Integration (Single Table)

# COMMAND ----------

def parse_weather_payload_enhanced(df: DataFrame) -> DataFrame:
    """Parse JSON payload and extract weather data (same as Bronze_to_Silver_Pipeline)"""
    
    print("üìä Parsing JSON payload to extract structured weather data...")
    
    # Parse JSON payload and extract weather fields (same logic as Bronze_to_Silver_Pipeline)
    parsed_df = df.select(
        # Inherited fields from Bronze
        col("record_id").alias("source_record_id"),
        col("ingestion_timestamp").alias("bronze_ingestion_timestamp"),
        col("ingestion_date").alias("bronze_ingestion_date"),
        col("ingestion_hour").alias("bronze_ingestion_hour"),
        
        # Event Hub technical fields (inherited)
        col("eventhub_partition"),
        col("eventhub_offset"),
        col("eventhub_sequence_number"),
        col("eventhub_enqueued_time"),
        col("eventhub_partition_key"),
        
        # Processing context (inherited)
        col("processing_cluster_id"),
        col("consumer_group"),
        col("eventhub_name"),
        
        # Original payload info
        col("payload_size_bytes").alias("original_payload_size_bytes"),
        col("raw_payload"),
        
        # Parse JSON fields from raw_payload (WeatherReading model - same as Bronze_to_Silver_Pipeline)
        get_json_object(col("raw_payload"), "$.event_id").alias("event_id"),
        get_json_object(col("raw_payload"), "$.city").alias("city"),
        get_json_object(col("raw_payload"), "$.latitude").cast("double").alias("latitude"),
        get_json_object(col("raw_payload"), "$.longitude").cast("double").alias("longitude"),
        get_json_object(col("raw_payload"), "$.temperature").cast("double").alias("temperature"),
        get_json_object(col("raw_payload"), "$.humidity").cast("double").alias("humidity"),
        get_json_object(col("raw_payload"), "$.wind_speed").cast("double").alias("wind_speed"),
        get_json_object(col("raw_payload"), "$.pressure").cast("double").alias("pressure"),
        get_json_object(col("raw_payload"), "$.precipitation").cast("double").alias("precipitation"),
        get_json_object(col("raw_payload"), "$.cloud_cover").cast("double").alias("cloud_cover"),
        get_json_object(col("raw_payload"), "$.weather_condition").alias("weather_condition"),
        get_json_object(col("raw_payload"), "$.timestamp").cast("timestamp").alias("weather_timestamp"),
        get_json_object(col("raw_payload"), "$.data_source").alias("data_source"),
        get_json_object(col("raw_payload"), "$.cluster_id").alias("weather_cluster_id"),
        get_json_object(col("raw_payload"), "$.notebook_path").alias("notebook_path")
    )
    
    # Add parsing status (same logic as Bronze_to_Silver_Pipeline)
    parsed_df = parsed_df.withColumn(
        "parsing_status",
        when(
            col("event_id").isNotNull() & 
            col("city").isNotNull() & 
            col("temperature").isNotNull(),
            lit("SUCCESS")
        ).when(
            col("event_id").isNotNull() | 
            col("city").isNotNull() | 
            col("temperature").isNotNull(),
            lit("PARTIAL")
        ).otherwise(lit("FAILED"))
    )
    
    # Add parsing errors (same logic as Bronze_to_Silver_Pipeline)
    parsed_df = parsed_df.withColumn(
        "parsing_errors",
        when(col("parsing_status") == "FAILED", 
             array(lit("JSON parsing completely failed"))
        ).when(col("parsing_status") == "PARTIAL",
             array(lit("Some required fields missing from JSON"))
        ).otherwise(array())  # Empty array for SUCCESS
    )
    
    # Handle missing values (same logic as Bronze_to_Silver_Pipeline)
    parsed_df = parsed_df.withColumn(
        "event_id",
        coalesce(
            col("event_id"),
            concat(
                coalesce(col("city"), lit("unknown")),
                lit("-"),
                unix_timestamp(col("weather_timestamp")).cast("string")
            )
        )
    )
    
    parsed_df = parsed_df.withColumn(
        "weather_timestamp",
        coalesce(col("weather_timestamp"), col("bronze_ingestion_timestamp"))
    )
    
    parsed_df = parsed_df.withColumn(
        "data_source",
        coalesce(col("data_source"), lit("databricks_weather_simulator"))
    )
    
    return parsed_df

def apply_dqx_quality_validation_single_table(df: DataFrame) -> DataFrame:
    """Apply DQX quality validation and add quality metadata to ALL records (Single Table Approach)"""
    
    print("üîç Applying DQX quality validation - adding quality flags to ALL records...")
    
    if not DQX_AVAILABLE or not quality_rules:
        print("‚ö†Ô∏è DQX not available - using basic validation fallback")
        return apply_basic_quality_validation_single_table(df)
    
    try:
        # Apply DQX quality checks using the framework
        print(f"üìä Executing {len(quality_rules)} DQX quality rules...")
        
        # **IMPORTANT: Single table approach - do NOT split data**
        # Use DQEngine to apply checks and get quality results
        # Note: DQX framework might not have a method that doesn't split, so we might need to use validate_checks
        
        try:
            # Try to use DQX to validate and annotate records
            # If apply_checks_and_split exists, we'll rejoin the results
            valid_data, invalid_data = dq_engine.apply_checks_and_split(df, quality_rules)
            
            # Add DQX quality metadata to valid records
            valid_with_metadata = add_dqx_quality_metadata(valid_data, is_valid=True)
            
            # Add DQX quality metadata to invalid records
            invalid_with_metadata = add_dqx_quality_metadata(invalid_data, is_valid=False)
            
            # **Single Table Approach: Union valid and invalid data back together**
            all_records_with_quality = valid_with_metadata.unionByName(invalid_with_metadata)
            
            print("‚úÖ DQX quality validation completed - all records annotated with quality flags")
            return all_records_with_quality
            
        except AttributeError:
            # If apply_checks_and_split doesn't exist, use alternative approach
            print("‚ö†Ô∏è DQX apply_checks_and_split not available, using basic validation with DQX metadata")
            return apply_basic_quality_validation_single_table(df)
        
    except Exception as e:
        print(f"‚ùå DQX quality validation failed: {e}")
        print("üîÑ Falling back to basic quality validation...")
        return apply_basic_quality_validation_single_table(df)

def apply_basic_quality_validation_single_table(df: DataFrame) -> DataFrame:
    """Fallback basic quality validation when DQX is not available - Single Table Approach"""
    
    print("üîÑ Applying basic quality validation (DQX fallback) - single table approach...")
    
    # Basic validation conditions (same as Bronze_to_Silver_Pipeline logic)
    quality_condition = (
        col("event_id").isNotNull() &
        col("city").isNotNull() &
        col("weather_timestamp").isNotNull() &
        col("parsing_status").isin(["SUCCESS", "PARTIAL"])
    )
    
    # **Single Table Approach: Add quality metadata to ALL records based on conditions**
    df_with_quality = df.withColumn(
        "flag_check",
        when(quality_condition, lit("PASS")).otherwise(lit("FAIL"))
    ).withColumn(
        "description_failure",
        when(
            quality_condition, 
            lit(None).cast("string")
        ).otherwise(
            lit("Basic validation failed: missing required fields or parsing failed")
        )
    ).withColumn(
        "dqx_rule_results",
        when(
            quality_condition,
            array(lit("Basic validation passed"))
        ).otherwise(
            array(lit("Basic validation failed"))
        )
    ).withColumn(
        "dqx_quality_score",
        when(quality_condition, lit(0.8)).otherwise(lit(0.0))
    ).withColumn(
        "dqx_validation_timestamp", current_timestamp()
    ).withColumn(
        "dqx_lineage_id", 
        concat(lit("basic-"), lit(str(uuid.uuid4())))
    ).withColumn(
        "dqx_criticality_level", lit("basic")
    ).withColumn(
        "failed_rules_count",
        when(quality_condition, lit(0)).otherwise(lit(1))
    ).withColumn(
        "passed_rules_count",
        when(quality_condition, lit(1)).otherwise(lit(0))
    )
    
    return df_with_quality

def add_dqx_quality_metadata(df: DataFrame, is_valid: bool) -> DataFrame:
    """Add DQX-specific quality metadata fields"""
    
    # Generate unique DQX lineage ID
    dqx_lineage_id = f"dqx-{uuid.uuid4()}"
    
    if is_valid:
        return df.withColumn("flag_check", lit("PASS")) \
                .withColumn("description_failure", lit(None).cast("string")) \
                .withColumn("dqx_rule_results", array(lit("All DQX rules passed"))) \
                .withColumn("dqx_quality_score", lit(1.0)) \
                .withColumn("dqx_validation_timestamp", current_timestamp()) \
                .withColumn("dqx_lineage_id", lit(dqx_lineage_id)) \
                .withColumn("dqx_criticality_level", lit(QUALITY_CRITICALITY)) \
                .withColumn("failed_rules_count", lit(0)) \
                .withColumn("passed_rules_count", lit(len(quality_rules)))
    else:
        return df.withColumn("flag_check", lit("FAIL")) \
                .withColumn("description_failure", lit("DQX validation failed - one or more quality rules not met")) \
                .withColumn("dqx_rule_results", array(lit("DQX validation failed"))) \
                .withColumn("dqx_quality_score", lit(0.0)) \
                .withColumn("dqx_validation_timestamp", current_timestamp()) \
                .withColumn("dqx_lineage_id", lit(dqx_lineage_id)) \
                .withColumn("dqx_criticality_level", lit(QUALITY_CRITICALITY)) \
                .withColumn("failed_rules_count", lit(len(quality_rules))) \
                .withColumn("passed_rules_count", lit(0))

def add_silver_processing_metadata_enhanced(df: DataFrame) -> DataFrame:
    """Add Silver layer processing metadata (same as Bronze_to_Silver_Pipeline)"""
    
    return df.withColumn(
        "silver_processing_timestamp", current_timestamp()
    ).withColumn(
        "silver_processing_date", current_date()
    ).withColumn(
        "silver_processing_hour", hour(current_timestamp())
    )

def transform_bronze_to_silver_with_dqx_single_table(bronze_df: DataFrame) -> DataFrame:
    """Complete transformation from Bronze to Silver with DQX quality validation - Single Enhanced Table"""
    
    print("üîÑ Starting Bronze to Silver transformation with DQX quality validation (Single Table)...")
    
    # Step 1: Parse weather data from JSON payload (same as Bronze_to_Silver_Pipeline)
    parsed_df = parse_weather_payload_enhanced(bronze_df)
    
    # Step 2: **NEW** - Apply DQX quality validation and add quality metadata to ALL records
    df_with_quality = apply_dqx_quality_validation_single_table(parsed_df)
    
    # Step 3: Add Silver processing metadata 
    silver_df = add_silver_processing_metadata_enhanced(df_with_quality)
    
    # Step 4: Select final enhanced Silver schema fields
    silver_df = silver_df.select([col.name for col in silver_enhanced_schema.fields])
    
    print("‚úÖ Bronze to Silver DQX transformation completed (Single Enhanced Table)")
    print("   All records (valid + invalid) included with quality flags")
    return silver_df

print("‚úÖ Enhanced weather data transformation functions with DQX integration defined (Single Table Approach)")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 6. Enhanced Hive Metastore Management (Single Enhanced Silver Table)

# COMMAND ----------

class EnhancedSilverHiveMetastoreManager:
    """Manage Enhanced Single Silver Hive Metastore table with DQX support"""
    
    def __init__(self, silver_database: str, silver_table: str, storage_account: str, container: str):
        self.silver_database = silver_database
        self.silver_table = silver_table
        self.silver_full_table = f"{silver_database}.{silver_table}"
        self.storage_account = storage_account
        self.container = container
    
    def create_database_if_not_exists(self):
        """Create database if it doesn't exist"""
        try:
            spark.sql(f"CREATE DATABASE IF NOT EXISTS {self.silver_database}")
            spark.sql(f"USE {self.silver_database}")
            print(f"‚úÖ Database '{self.silver_database}' ensured and selected")
        except Exception as e:
            print(f"‚ö†Ô∏è Error creating database: {e}")
    
    def create_enhanced_silver_table(self, table_path: str):
        """Create Enhanced Single Silver Delta table with DQX quality metadata"""
        try:
            table_exists = self._table_exists(self.silver_full_table)
            
            if not table_exists:
                print(f"üìù Creating Enhanced Silver table with DQX metadata: {self.silver_full_table}")
                
                # Build CREATE TABLE DDL for Enhanced Silver with DQX fields
                ddl_columns = []
                for field in silver_enhanced_schema.fields:
                    spark_type = field.dataType.simpleString()
                    nullable = "" if field.nullable else "NOT NULL"
                    ddl_columns.append(f"`{field.name}` {spark_type} {nullable}")
                
                columns_ddl = ",\n  ".join(ddl_columns)
                
                create_table_sql = f"""
                CREATE TABLE {self.silver_full_table} (
                  {columns_ddl}
                )
                USING DELTA
                LOCATION '{table_path}'
                PARTITIONED BY (silver_processing_date, silver_processing_hour)
                TBLPROPERTIES (
                  'delta.autoOptimize.optimizeWrite' = 'true',
                  'delta.autoOptimize.autoCompact' = 'true',
                  'delta.logRetentionDuration' = '30 days',
                  'delta.deletedFileRetentionDuration' = '7 days',
                  'description' = 'Enhanced Silver layer with DQX quality validation - Single Table',
                  'data_source' = 'Bronze layer with DQX quality framework',
                  'ingestion_pattern' = 'Streaming Bronze to Silver with DQX',
                  'data_format' = 'Weather data with DQX quality metadata - ALL records',
                  'quality_framework' = 'Databricks DQX',
                  'storage_account' = '{self.storage_account}',
                  'container' = '{self.container}',
                  'dqx_enabled' = 'true',
                  'quality_criticality' = '{QUALITY_CRITICALITY}',
                  'quality_threshold' = '{QUALITY_THRESHOLD}',
                  'table_approach' = 'single_table_with_quality_flags'
                )
                """
                
                spark.sql(create_table_sql)
                print(f"‚úÖ Enhanced Single Silver table created: {self.silver_full_table}")
            else:
                print(f"‚úÖ Enhanced Single Silver table already exists: {self.silver_full_table}")
            
            return True
            
        except Exception as e:
            print(f"‚ùå Error creating Enhanced Single Silver table: {e}")
            return False
    
    def _table_exists(self, table_name: str) -> bool:
        """Check if table exists in Hive Metastore"""
        try:
            spark.sql(f"DESCRIBE TABLE {table_name}")
            return True
        except:
            return False
    
    def show_enhanced_single_table_info(self):
        """Display Enhanced Single table information with DQX metadata"""
        try:
            print(f"üìä Enhanced Single Silver Table Information: {self.silver_full_table}")
            print("=" * 60)
            
            # Show Silver table details
            table_info = spark.sql(f"DESCRIBE DETAIL {self.silver_full_table}").collect()[0]
            
            print(f"üìÅ Location: {table_info['location']}")
            print(f"üìã Format: {table_info['format']}")
            print(f"üìä Number of Files: {table_info['numFiles']}")
            print(f"üíæ Size in Bytes: {table_info['sizeInBytes']:,}")
            print(f"üïí Created: {table_info['createdAt']}")
            print(f"üîÑ Last Modified: {table_info['lastModified']}")
            
            # Show quality distribution if table has data
            try:
                total_records = spark.table(self.silver_full_table).count()
                if total_records > 0:
                    quality_distribution = spark.table(self.silver_full_table) \
                        .groupBy("flag_check") \
                        .count() \
                        .collect()
                    
                    print(f"\nüìä Quality Status Distribution (Total: {total_records:,}):")
                    for row in quality_distribution:
                        print(f"   {row['flag_check']}: {row['count']:,} records")
                        
            except Exception as dist_error:
                print(f"üìä Quality distribution: Not available yet")
            
        except Exception as e:
            print(f"‚ùå Error retrieving Enhanced Single table info: {e}")

# Initialize Enhanced Hive Metastore Manager (Single Table)
enhanced_hive_manager = EnhancedSilverHiveMetastoreManager(
    SILVER_DATABASE, SILVER_TABLE, STORAGE_ACCOUNT, CONTAINER
)
print(f"‚úÖ Enhanced Single Silver Hive Metastore manager initialized")
print(f"   Enhanced Silver Table: {SILVER_FULL_TABLE} (Single table with quality flags)")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 7. Enhanced ADLS Configuration

# COMMAND ----------

class EnhancedADLSConfig:
    """Enhanced ADLS Gen2 configuration for Single Silver layer"""
    
    def __init__(self, storage_account: str, container: str):
        self.storage_account = storage_account
        self.container = container
        self.base_path = f"abfss://{container}@{storage_account}.dfs.core.windows.net"
        
        # Check mount point
        try:
            dbutils.fs.ls(f"/mnt/datalake/")
            self.is_mounted = True
            print("‚úÖ ADLS Gen2 storage is mounted")
        except:
            self.is_mounted = False
            print("‚ö†Ô∏è ADLS Gen2 storage mount not found - using direct abfss:// paths")
    
    def get_path(self, base_path: str) -> str:
        """Get full path for any base path"""
        if self.is_mounted:
            return base_path
        else:
            relative_path = base_path.replace("/mnt/datalake/", "")
            return f"{self.base_path}/{relative_path}"

# Initialize Enhanced ADLS configuration
enhanced_adls_config = EnhancedADLSConfig(STORAGE_ACCOUNT, CONTAINER)
silver_enhanced_path = enhanced_adls_config.get_path(SILVER_PATH)
checkpoint_enhanced_path = enhanced_adls_config.get_path(CHECKPOINT_PATH)

print(f"‚úÖ Enhanced ADLS Gen2 configuration (Single Table):")
print(f"   Storage Account: {STORAGE_ACCOUNT}")
print(f"   Container: {CONTAINER}")
print(f"   Enhanced Silver Path: {silver_enhanced_path}")
print(f"   Checkpoint Path: {checkpoint_enhanced_path}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 8. Setup Enhanced Database and Single Silver Table

# COMMAND ----------

def setup_enhanced_single_hive_components():
    """Setup Enhanced database and single Silver table with DQX support"""
    
    print("üèóÔ∏è Setting up Enhanced Single Silver Hive Metastore components with DQX support...")
    print("=" * 50)
    
    # Create database if not exists
    enhanced_hive_manager.create_database_if_not_exists()
    
    # Create Enhanced Single Silver table with DQX metadata
    silver_success = enhanced_hive_manager.create_enhanced_silver_table(silver_enhanced_path)
    
    if silver_success:
        print("‚úÖ Enhanced Single Silver Hive Metastore setup completed successfully")
        print(f"üìä Enhanced Single Silver table: {SILVER_FULL_TABLE}")
        print("   Approach: Single table with quality flags (flag_check: PASS/FAIL)")
        
        # Show enhanced table information
        enhanced_hive_manager.show_enhanced_single_table_info()
        
        return True
    else:
        print("‚ùå Enhanced Single Silver Hive Metastore setup failed")
        return False

# Setup Enhanced Single Silver Hive Metastore components
enhanced_single_setup_success = setup_enhanced_single_hive_components()

# COMMAND ----------

# MAGIC %md
# MAGIC ## 9. Enhanced Streaming Pipeline with DQX Quality Framework (Single Table)

# COMMAND ----------

def get_trigger_config(trigger_mode: str):
    """Get trigger configuration (same as Bronze_to_Silver_Pipeline)"""
    if "second" in trigger_mode:
        interval = trigger_mode.split()[0]
        return {"processingTime": f"{interval} seconds"}
    elif "minute" in trigger_mode:
        interval = trigger_mode.split()[0]
        return {"processingTime": f"{interval} minutes"}
    else:
        return {"processingTime": "5 seconds"}

def start_enhanced_bronze_to_silver_dqx_single_table_streaming():
    """Start the Enhanced Bronze to Silver streaming pipeline with DQX quality framework (Single Table)"""
    
    if not enhanced_single_setup_success:
        print("‚ùå Cannot start enhanced pipeline - database setup failed")
        return None
    
    print("üöÄ Starting Enhanced Bronze to Silver DQX Streaming Pipeline (Single Table)")
    print("=" * 60)
    print(f"üìä Source: {BRONZE_FULL_TABLE}")
    print(f"‚ú® Enhanced Single Silver Target: {SILVER_FULL_TABLE}")
    print(f"üìã Approach: Single table with quality flags (ALL records)")
    print(f"üîç Quality Framework: {'Databricks DQX' if DQX_AVAILABLE else 'Basic Validation'}")
    print(f"üìè Quality Threshold: {QUALITY_THRESHOLD}")
    print(f"‚ö° Trigger: {TRIGGER_MODE}")
    print(f"üì¶ Max Events/Trigger: {MAX_EVENTS_PER_TRIGGER}")
    print(f"üè™ Storage Account: {STORAGE_ACCOUNT}")
    print(f"üìÅ Container: {CONTAINER}")
    print(f"üîÑ Checkpoint: {checkpoint_enhanced_path}")
    print(f"‚è±Ô∏è Checkpoint Interval: 30 seconds")
    print(f"üìã DQX Rules: {len(quality_rules)} quality validation rules")
    print("=" * 60)
    
    try:
        # Read from Bronze table as stream
        print("üìñ Reading Bronze table stream...")
        bronze_stream = spark \
            .readStream \
            .format("delta") \
            .table(BRONZE_FULL_TABLE)
        
        print("‚úÖ Bronze stream source created")
        
        # Apply transformation to Enhanced Single Silver with DQX
        print("üîÑ Applying Bronze to Silver transformation with DQX (Single Table)...")
        silver_stream = transform_bronze_to_silver_with_dqx_single_table(bronze_stream)
        
        print("‚úÖ Enhanced Single Silver stream transformation with DQX configured")
        
        # Configure trigger
        trigger_config = get_trigger_config(TRIGGER_MODE)
        print(f"‚úÖ Trigger configured: {trigger_config}")
        
        # Write to Enhanced Single Silver Hive Metastore table with checkpointing
        query = silver_stream \
            .writeStream \
            .outputMode("append") \
            .format("delta") \
            .option("checkpointLocation", checkpoint_enhanced_path) \
            .option("maxRecordsPerFile", "50000") \
            .trigger(**trigger_config) \
            .toTable(SILVER_FULL_TABLE)
        
        print("‚úÖ Enhanced Bronze to Silver DQX pipeline (Single Table) started successfully")
        print(f"üìä Query ID: {query.id}")
        print(f"üîÑ Run ID: {query.runId}")
        print(f"üìù Mode: APPEND-ONLY")
        print(f"üóÑÔ∏è Writing to Enhanced Single Silver Hive table: {SILVER_FULL_TABLE}")
        print(f"üîç Using: {'Databricks DQX Framework' if DQX_AVAILABLE else 'Basic Validation Fallback'}")
        print(f"üìã DQX Quality Rules: {len(quality_rules)}")
        print(f"üéØ Quality Approach: Single table with flag_check field (PASS/FAIL)")
        
        return query
        
    except Exception as e:
        print(f"‚ùå Failed to start Enhanced DQX Single Table pipeline: {e}")
        raise

print("‚úÖ Enhanced Bronze to Silver DQX single table streaming functions defined")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 10. Start the Enhanced DQX Single Table Pipeline

# COMMAND ----------

# Start the Enhanced Bronze to Silver DQX streaming pipeline (Single Table)
enhanced_dqx_single_streaming_query = start_enhanced_bronze_to_silver_dqx_single_table_streaming()

# COMMAND ----------

# MAGIC %md
# MAGIC ## 11. Enhanced DQX Single Table Pipeline Monitoring

# COMMAND ----------

import time

class EnhancedSingleTableDQXPipelineMonitor:
    """Monitor Enhanced DQX single table pipeline with comprehensive quality metrics"""
    
    def __init__(self, silver_table: str):
        self.silver_table = silver_table
    
    def display_enhanced_single_table_dqx_dashboard(self, query: StreamingQuery):
        """Display comprehensive single table DQX quality dashboard"""
        try:
            # Get quality metrics from single enhanced table
            total_records = spark.table(self.silver_table).count()
            
            if total_records > 0:
                # Quality distribution
                quality_stats = spark.table(self.silver_table) \
                    .groupBy("flag_check") \
                    .count() \
                    .collect()
                
                pass_count = 0
                fail_count = 0
                for row in quality_stats:
                    if row["flag_check"] == "PASS":
                        pass_count = row["count"]
                    elif row["flag_check"] == "FAIL":
                        fail_count = row["count"]
                
                quality_rate = (pass_count / total_records) if total_records > 0 else 0
                
                # Get average quality score
                avg_quality_score = 0.0
                avg_score_row = spark.table(self.silver_table) \
                    .select(avg("dqx_quality_score").alias("avg_score")).collect()[0]
                avg_quality_score = avg_score_row["avg_score"] if avg_score_row["avg_score"] else 0.0
            else:
                pass_count = 0
                fail_count = 0
                quality_rate = 0
                avg_quality_score = 0.0
            
            # Recent processing metrics
            recent_records = spark.table(self.silver_table) \
                .filter("silver_processing_date >= current_date()") \
                .count()
            
            progress = query.lastProgress if query else None
            batch_id = progress.get('batchId', 0) if progress else 0
            
            dashboard_html = f"""
            <div style="border: 2px solid #4CAF50; border-radius: 15px; padding: 20px; background: #f8f9fa;">
                <h2 style="color: #4CAF50; text-align: center;">üîç Enhanced Single Table DQX Pipeline Dashboard</h2>
                
                <div style="background: #e8f5e8; padding: 15px; border-radius: 10px; border-left: 5px solid #4CAF50; margin: 15px 0;">
                    <h4 style="margin: 0; color: #2e7d2e;">üìä Single Table Approach</h4>
                    <p style="margin: 5px 0; color: #2e7d2e;"><strong>ALL records</strong> stored in same table with quality flags (flag_check: PASS/FAIL)</p>
                </div>
                
                <div style="display: grid; grid-template-columns: repeat(4, 1fr); gap: 15px; margin: 20px 0;">
                    <div style="text-align: center; padding: 15px; background: white; border-radius: 10px; border-left: 5px solid #4CAF50;">
                        <h4 style="margin: 0; color: #4CAF50;">Total Records</h4>
                        <p style="font-size: 28px; margin: 5px 0; font-weight: bold;">{total_records:,}</p>
                        <p style="margin: 0; color: #666;">Enhanced Silver</p>
                    </div>
                    <div style="text-align: center; padding: 15px; background: white; border-radius: 10px; border-left: 5px solid #2196F3;">
                        <h4 style="margin: 0; color: #2196F3;">DQX PASS</h4>
                        <p style="font-size: 28px; margin: 5px 0; font-weight: bold;">{pass_count:,}</p>
                        <p style="margin: 0; color: #666;">Quality Validated</p>
                    </div>
                    <div style="text-align: center; padding: 15px; background: white; border-radius: 10px; border-left: 5px solid #FF9800;">
                        <h4 style="margin: 0; color: #FF9800;">DQX FAIL</h4>
                        <p style="font-size: 28px; margin: 5px 0; font-weight: bold;">{fail_count:,}</p>
                        <p style="margin: 0; color: #666;">Quality Failed</p>
                    </div>
                    <div style="text-align: center; padding: 15px; background: white; border-radius: 10px; border-left: 5px solid #9C27B0;">
                        <h4 style="margin: 0; color: #9C27B0;">DQX Quality Rate</h4>
                        <p style="font-size: 28px; margin: 5px 0; font-weight: bold;">{quality_rate:.1%}</p>
                        <p style="margin: 0; color: #666;">Success Rate</p>
                    </div>
                </div>
                
                <div style="background: #e3f2fd; padding: 15px; border-radius: 10px; margin-top: 20px;">
                    <h4 style="margin: 0 0 10px 0;">üìä Enhanced Single Table DQX Framework Status</h4>
                    <strong>Framework:</strong> {'‚úÖ Databricks DQX' if DQX_AVAILABLE else '‚ö†Ô∏è Basic Validation'} |
                    <strong>Rules:</strong> {len(quality_rules)} quality checks |
                    <strong>Criticality:</strong> {QUALITY_CRITICALITY} |
                    <strong>Avg Score:</strong> {avg_quality_score:.2f} |
                    <strong>Batch:</strong> {batch_id} |
                    <strong>Status:</strong> {'üü¢ Active' if query and query.isActive else 'üî¥ Stopped'}
                </div>
                
                <div style="background: #f3e5f5; padding: 15px; border-radius: 10px; margin-top: 15px;">
                    <h4 style="margin: 0 0 10px 0;">üìà Today's Processing</h4>
                    <strong>Today's Records:</strong> {recent_records:,} |
                    <strong>Table:</strong> {self.silver_table} |
                    <strong>Approach:</strong> Single Enhanced Table
                </div>
            </div>
            """
            
            displayHTML(dashboard_html)
            
        except Exception as e:
            print(f"‚ö†Ô∏è Enhanced Single Table DQX Dashboard error: {e}")
    
    def show_enhanced_single_table_quality_analysis(self):
        """Show comprehensive single table DQX quality analysis"""
        
        try:
            print("üìä Enhanced Single Table Databricks DQX Quality Analysis")
            print("=" * 50)
            
            total_records = spark.table(self.silver_table).count()
            
            if total_records > 0:
                print("‚ú® Enhanced Single Silver Table DQX Quality Metrics:")
                
                # Overall quality metrics
                spark.table(self.silver_table) \
                    .select(
                        count("*").alias("total_records"),
                        sum(when(col("flag_check") == "PASS", 1).otherwise(0)).alias("pass_count"),
                        sum(when(col("flag_check") == "FAIL", 1).otherwise(0)).alias("fail_count"),
                        avg("dqx_quality_score").alias("avg_dqx_score"),
                        min("dqx_quality_score").alias("min_dqx_score"),
                        max("dqx_quality_score").alias("max_dqx_score")
                    ).show()
                
                print("\nüìã Quality Status Distribution:")
                spark.table(self.silver_table) \
                    .groupBy("flag_check", "dqx_criticality_level") \
                    .count() \
                    .orderBy("flag_check", "dqx_criticality_level") \
                    .show()
                
                print("\nüìÖ Quality Processing by Date:")
                spark.table(self.silver_table) \
                    .groupBy("silver_processing_date") \
                    .agg(
                        count("*").alias("total_records"),
                        sum(when(col("flag_check") == "PASS", 1).otherwise(0)).alias("pass_count"),
                        sum(when(col("flag_check") == "FAIL", 1).otherwise(0)).alias("fail_count"),
                        avg("dqx_quality_score").alias("avg_quality_score")
                    ) \
                    .withColumn("quality_rate", (col("pass_count") / col("total_records")) * 100) \
                    .orderBy("silver_processing_date") \
                    .show(10)
                
                # Failed records analysis
                failed_count = spark.table(self.silver_table) \
                    .filter("flag_check = 'FAIL'") \
                    .count()
                
                if failed_count > 0:
                    print(f"\nüö´ Quality Failures Analysis ({failed_count:,} failed records):")
                    spark.table(self.silver_table) \
                        .filter("flag_check = 'FAIL'") \
                        .groupBy("description_failure") \
                        .count() \
                        .orderBy(col("count").desc()) \
                        .show(10, truncate=False)
                    
                    print("\nüìã Sample Failed Records:")
                    spark.table(self.silver_table) \
                        .filter("flag_check = 'FAIL'") \
                        .select("event_id", "city", "description_failure", 
                               "dqx_quality_score", "silver_processing_timestamp") \
                        .orderBy(col("silver_processing_timestamp").desc()) \
                        .show(5, truncate=False)
                else:
                    print("‚úÖ No quality failures found - excellent data quality!")
                    
            else:
                print("üì≠ No Enhanced Single Silver data found yet")
                
        except Exception as e:
            print(f"‚ùå Enhanced Single Table Quality analysis error: {e}")
    
    def show_dqx_lineage_tracking_single_table(self):
        """Show DQX lineage tracking information for single table"""
        
        try:
            print("üîó Single Table DQX Lineage Tracking Analysis")
            print("=" * 40)
            
            total_records = spark.table(self.silver_table).count()
            
            if total_records > 0:
                # Sample DQX lineage IDs
                print("üìã Sample DQX Lineage IDs (Both PASS and FAIL):")
                spark.table(self.silver_table) \
                    .select("dqx_lineage_id", "event_id", "flag_check", 
                           "dqx_quality_score", "dqx_validation_timestamp") \
                    .orderBy("dqx_validation_timestamp") \
                    .show(10, truncate=False)
            
        except Exception as e:
            print(f"‚ö†Ô∏è Single Table DQX Lineage tracking error: {e}")

# Initialize Enhanced Single Table DQX Monitor
enhanced_single_dqx_monitor = EnhancedSingleTableDQXPipelineMonitor(SILVER_FULL_TABLE)

# Monitor the Enhanced Single Table DQX pipeline
if enhanced_dqx_single_streaming_query:
    print("üìä Monitoring Enhanced Single Table Bronze to Silver DQX Pipeline")
    print(f"üîç Quality Framework: {'Databricks DQX' if DQX_AVAILABLE else 'Basic Validation Fallback'}")
    print(f"üìã DQX Quality Rules: {len(quality_rules)}")
    print(f"üéØ Approach: Single Enhanced Table with quality flags")
    print(f"üè™ Storage Account: {STORAGE_ACCOUNT}")
    print(f"üìÅ Container: {CONTAINER}")
    print("=" * 60)
    
    try:
        # Monitor for a few iterations
        for i in range(3):  # Monitor for 3 iterations
            if enhanced_dqx_single_streaming_query.isActive:
                print(f"\nüîÑ Enhanced Single Table DQX Monitoring Iteration {i+1}")
                
                # Display Enhanced Single Table DQX dashboard
                enhanced_single_dqx_monitor.display_enhanced_single_table_dqx_dashboard(enhanced_dqx_single_streaming_query)
                
                # Show progress
                try:
                    progress = enhanced_dqx_single_streaming_query.lastProgress
                    if progress:
                        batch_id = progress.get('batchId', 0)
                        print(f"üìä Enhanced Single Table DQX Last Batch ID: {batch_id}")
                except:
                    pass
                
                time.sleep(20)  # Wait 20 seconds between checks
            else:
                print("‚ùå Enhanced Single Table DQX pipeline is not active")
                break
        
        if enhanced_dqx_single_streaming_query.isActive:
            print("‚úÖ Enhanced Single Table DQX pipeline running successfully in background")
            enhanced_single_dqx_monitor.show_enhanced_single_table_quality_analysis()
            enhanced_single_dqx_monitor.show_dqx_lineage_tracking_single_table()
        
    except KeyboardInterrupt:
        print("‚èπÔ∏è Enhanced Single Table DQX Monitoring interrupted")
    except Exception as e:
        print(f"‚ö†Ô∏è Enhanced Single Table DQX Monitoring error: {e}")
else:
    print("‚ùå Enhanced Single Table DQX pipeline not started")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 12. Enhanced Single Table Pipeline Management Commands

# COMMAND ----------

def check_enhanced_single_table_dqx_pipeline_status():
    """Check Enhanced Single Table DQX pipeline status"""
    if enhanced_dqx_single_streaming_query:
        print(f"üìä Enhanced Single Table Databricks DQX Pipeline Status:")
        print(f"   Query ID: {enhanced_dqx_single_streaming_query.id}")
        print(f"   Run ID: {enhanced_dqx_single_streaming_query.runId}")
        print(f"   Active: {enhanced_dqx_single_streaming_query.isActive}")
        print(f"   DQX Framework: {'Available' if DQX_AVAILABLE else 'Not Available (using fallback)'}")
        print(f"   DQX Rules Count: {len(quality_rules)}")
        print(f"   Quality Criticality: {QUALITY_CRITICALITY}")
        print(f"   Quality Threshold: {QUALITY_THRESHOLD:.1%}")
        print(f"   Source: {BRONZE_FULL_TABLE}")
        print(f"   Enhanced Single Silver: {SILVER_FULL_TABLE}")
        print(f"   Approach: Single table with quality flags (flag_check: PASS/FAIL)")
        print(f"   Configuration: Extends Bronze_to_Silver_Pipeline")
        
        try:
            progress = enhanced_dqx_single_streaming_query.lastProgress
            if progress:
                print(f"   Last Batch: {progress.get('batchId', 'N/A')}")
        except:
            pass
        
        return True
    else:
        print("‚ùå No Enhanced Single Table DQX pipeline found")
        return False

def stop_enhanced_single_table_dqx_pipeline():
    """Stop the Enhanced Single Table DQX pipeline"""
    try:
        if enhanced_dqx_single_streaming_query and enhanced_dqx_single_streaming_query.isActive:
            print("‚èπÔ∏è Stopping Enhanced Single Table Databricks DQX pipeline...")
            enhanced_dqx_single_streaming_query.stop()
            
            # Wait for graceful shutdown
            timeout = 30
            elapsed = 0
            while enhanced_dqx_single_streaming_query.isActive and elapsed < timeout:
                time.sleep(1)
                elapsed += 1
            
            if not enhanced_dqx_single_streaming_query.isActive:
                print("‚úÖ Enhanced Single Table DQX pipeline stopped successfully")
                enhanced_single_dqx_monitor.show_enhanced_single_table_quality_analysis()
            else:
                print("‚ö†Ô∏è Enhanced Single Table DQX Pipeline stop timeout")
        else:
            print("‚ÑπÔ∏è Enhanced Single Table DQX pipeline is already stopped")
    except Exception as e:
        print(f"‚ùå Error stopping Enhanced Single Table DQX pipeline: {e}")

def show_enhanced_single_table_dqx():
    """Show Enhanced Single Table DQX Silver table statistics"""
    print("üìä Enhanced Single Table Databricks DQX Pipeline:")
    print("=" * 50)
    
    try:
        print(f"‚ú® Enhanced Single Silver Table: {SILVER_FULL_TABLE}")
        total_count = spark.table(SILVER_FULL_TABLE).count()
        print(f"   Total Records: {total_count:,}")
        
        if total_count > 0:
            # Quality distribution
            quality_distribution = spark.table(SILVER_FULL_TABLE) \
                .groupBy("flag_check") \
                .count() \
                .collect()
            
            pass_count = 0
            fail_count = 0
            for row in quality_distribution:
                print(f"   {row['flag_check']}: {row['count']:,} records")
                if row["flag_check"] == "PASS":
                    pass_count = row["count"]
                elif row["flag_check"] == "FAIL":
                    fail_count = row["count"]
            
            # Quality score statistics
            quality_stats = spark.table(SILVER_FULL_TABLE) \
                .select(
                    avg("dqx_quality_score").alias("avg_score"),
                    min("dqx_quality_score").alias("min_score"),
                    max("dqx_quality_score").alias("max_score")
                ).collect()[0]
            
            print(f"   Avg DQX Quality Score: {quality_stats['avg_score']:.3f}")
            print(f"   Quality Score Range: {quality_stats['min_score']:.3f} - {quality_stats['max_score']:.3f}")
            
            # Quality rate
            quality_rate = (pass_count / total_count) if total_count > 0 else 0
            print(f"   Overall DQX Quality Rate: {quality_rate:.2%}")
            print(f"   Quality vs Threshold: {'‚úÖ Above' if quality_rate >= QUALITY_THRESHOLD else '‚ö†Ô∏è Below'} {QUALITY_THRESHOLD:.1%}")
            
            # Recent records sample
            print(f"\nüïí Recent Enhanced Single Silver Records:")
            spark.table(SILVER_FULL_TABLE) \
                .orderBy(col("silver_processing_timestamp").desc()) \
                .select("event_id", "city", "temperature", "flag_check", 
                       "dqx_quality_score", "description_failure", "silver_processing_timestamp") \
                .show(5, truncate=False)
        
        print(f"\nüîß Enhanced Single Table Configuration Summary:")
        print(f"   Storage Account: {STORAGE_ACCOUNT}")
        print(f"   Container: {CONTAINER}")
        print(f"   Secret Scope: {SECRET_SCOPE}")
        print(f"   DQX Framework: {'‚úÖ Available' if DQX_AVAILABLE else '‚ùå Using Fallback'}")
        print(f"   DQX Rules: {len(quality_rules)}")
        print(f"   Quality Criticality: {QUALITY_CRITICALITY}")
        print(f"   Approach: Single Enhanced Table (ALL records with quality flags)")
        
    except Exception as e:
        print(f"‚ùå Error checking Enhanced Single Table DQX: {e}")

# Check Enhanced Single Table DQX status
check_enhanced_single_table_dqx_pipeline_status()
print()
show_enhanced_single_table_dqx()

# COMMAND ----------

# MAGIC %md
# MAGIC ## 13. Final Summary and Single Table Extension Documentation

# COMMAND ----------

def display_enhanced_single_table_pipeline_summary():
    """Display comprehensive Enhanced Single Table DQX pipeline summary"""
    
    summary_html = f"""
    <div style="border: 2px solid #4CAF50; border-radius: 15px; padding: 20px; background: #f8f9fa;">
        <h2 style="color: #4CAF50; text-align: center;">üîç Enhanced Bronze to Silver Pipeline with DQX (Single Table)</h2>
        
        <div style="background: #d4edda; padding: 15px; border-radius: 10px; border-left: 5px solid #28a745; margin: 20px 0;">
            <h3 style="color: #155724; margin-top: 0;">üìà Extension of Bronze_to_Silver_Pipeline (Single Table Approach)</h3>
            <p style="margin: 0; color: #155724;">This class <strong>extends</strong> Bronze_to_Silver_Pipeline.py with DQX quality framework using <strong>single enhanced table</strong></p>
        </div>
        
        <div style="background: #cce5ff; padding: 15px; border-radius: 10px; border-left: 5px solid #0066cc; margin: 20px 0;">
            <h3 style="color: #003d7a; margin-top: 0;">üéØ Single Table Approach</h3>
            <p style="margin: 0; color: #003d7a;"><strong>ALL records</strong> (valid + invalid) saved to same table: <code>{SILVER_FULL_TABLE}</code></p>
            <p style="margin: 5px 0; color: #003d7a;">Quality status tracked via <strong>flag_check</strong> field: PASS/FAIL</p>
        </div>
        
        <div style="display: grid; grid-template-columns: repeat(2, 1fr); gap: 20px; margin: 20px 0;">
            <div style="background: white; padding: 15px; border-radius: 10px; border-left: 5px solid #2196F3;">
                <h3 style="color: #2196F3; margin-top: 0;">üîß Inherited Configuration</h3>
                <ul style="margin: 0;">
                    <li><strong>Secret Scope:</strong> {SECRET_SCOPE}</li>
                    <li><strong>Storage Account:</strong> {STORAGE_ACCOUNT}</li>
                    <li><strong>Container:</strong> {CONTAINER}</li>
                    <li><strong>Source:</strong> {BRONZE_FULL_TABLE}</li>
                    <li><strong>Trigger:</strong> {TRIGGER_MODE}</li>
                    <li><strong>Max Events:</strong> {MAX_EVENTS_PER_TRIGGER}</li>
                </ul>
            </div>
            
            <div style="background: white; padding: 15px; border-radius: 10px; border-left: 5px solid #FF9800;">
                <h3 style="color: #FF9800; margin-top: 0;">üìä DQX Single Table Enhancement</h3>
                <ul style="margin: 0;">
                    <li><strong>Framework:</strong> {'Databricks DQX' if DQX_AVAILABLE else 'Basic Validation'}</li>
                    <li><strong>Quality Rules:</strong> {len(quality_rules)} validations</li>
                    <li><strong>Enhanced Silver:</strong> {SILVER_FULL_TABLE}</li>
                    <li><strong>Approach:</strong> Single Enhanced Table</li>
                    <li><strong>Criticality:</strong> {QUALITY_CRITICALITY}</li>
                    <li><strong>Threshold:</strong> {QUALITY_THRESHOLD:.1%}</li>
                </ul>
            </div>
        </div>
        
        <div style="background: white; padding: 15px; border-radius: 10px; border-left: 5px solid #9C27B0; margin: 20px 0;">
            <h3 style="color: #9C27B0; margin-top: 0;">‚ú® NEW: DQX Quality Metadata Fields (Added to Same Table)</h3>
            <div style="display: grid; grid-template-columns: repeat(3, 1fr); gap: 10px; font-family: monospace; font-size: 12px;">
                <div><strong>flag_check:</strong> PASS/FAIL/WARNING</div>
                <div><strong>description_failure:</strong> Failure details</div>
                <div><strong>dqx_rule_results:</strong> Rule execution results</div>
                <div><strong>dqx_quality_score:</strong> Overall quality score (0-1)</div>
                <div><strong>dqx_validation_timestamp:</strong> Validation time</div>
                <div><strong>dqx_lineage_id:</strong> Lineage tracking ID</div>
                <div><strong>dqx_criticality_level:</strong> error/warn level</div>
                <div><strong>failed_rules_count:</strong> Number of failed rules</div>
                <div><strong>passed_rules_count:</strong> Number of passed rules</div>
            </div>
        </div>
        
        <div style="background: white; padding: 15px; border-radius: 10px; border-left: 5px solid #795548; margin: 20px 0;">
            <h3 style="color: #795548; margin-top: 0;">üîç DQX Quality Rules Implemented</h3>
            <div style="display: grid; grid-template-columns: repeat(2, 1fr); gap: 15px;">
                <div>
                    <h4>Completeness Checks:</h4>
                    <ul style="margin: 5px 0; font-size: 14px;">
                        <li>event_id not null/empty</li>
                        <li>city not null/empty</li>
                        <li>weather_timestamp not null</li>
                    </ul>
                </div>
                <div>
                    <h4>Type & Range Validation:</h4>
                    <ul style="margin: 5px 0; font-size: 14px;">
                        <li>Temperature: -60¬∞C to 60¬∞C</li>
                        <li>Humidity: 0% to 100%</li>
                        <li>Coordinates: Valid lat/lon ranges</li>
                    </ul>
                </div>
                <div>
                    <h4>Format Validation:</h4>
                    <ul style="margin: 5px 0; font-size: 14px;">
                        <li>City name length (2-50 chars)</li>
                        <li>Data source validation</li>
                        <li>Timestamp consistency</li>
                    </ul>
                </div>
                <div>
                    <h4>Single Table Benefits:</h4>
                    <ul style="margin: 5px 0; font-size: 14px;">
                        <li>ALL records in one place</li>
                        <li>Simple quality analysis</li>
                        <li>No data splitting required</li>
                    </ul>
                </div>
            </div>
        </div>
    </div>
    """
    
    displayHTML(summary_html)

def show_dqx_single_table_extension_summary():
    """Show DQX single table extension summary"""
    
    print("üîß Enhanced DQX Single Table Pipeline Extension Summary")
    print("=" * 50)
    print(f"Base Class: Bronze_to_Silver_Pipeline.py")
    print(f"Extension: Bronze_to_Silver_DQX_Enhanced_Pipeline.py")
    print()
    print("üìä What's New in the Enhanced Single Table Version:")
    print("‚úÖ Databricks DQX Framework integration")
    print("‚úÖ Comprehensive quality rules (completeness, type, range, format)")
    print("‚úÖ Enhanced Silver schema with DQX quality metadata fields")
    print("‚úÖ **SINGLE TABLE approach** - ALL records with quality flags")
    print("‚úÖ Real-time quality monitoring and lineage tracking")
    print("‚úÖ Quality score calculation and failure analysis")
    print("‚úÖ Same configuration as Bronze_to_Silver_Pipeline")
    print()
    print("üéØ Single Table Approach Benefits:")
    print("   - ALL records (valid + invalid) in same table")
    print("   - Simple quality analysis with flag_check field")
    print("   - No data splitting or separate quarantine table")
    print("   - Easy filtering: WHERE flag_check = 'PASS' for valid records")
    print("   - Easy filtering: WHERE flag_check = 'FAIL' for invalid records")
    print()
    print("üîç DQX Quality Metadata Fields Added to Same Silver Table:")
    print("   - flag_check: PASS/FAIL indicator")
    print("   - description_failure: Detailed failure descriptions") 
    print("   - dqx_rule_results: Array of rule execution results")
    print("   - dqx_quality_score: Overall quality score (0.0-1.0)")
    print("   - dqx_validation_timestamp: When validation occurred")
    print("   - dqx_lineage_id: Unique lineage tracking identifier")
    print("   - dqx_criticality_level: error/warn criticality")
    print("   - failed_rules_count: Number of failed DQX rules")
    print("   - passed_rules_count: Number of passed DQX rules")
    print()
    print("üìã Quality Rules Applied:")
    print(f"   Total Rules: {len(quality_rules)}")
    print(f"   Criticality: {QUALITY_CRITICALITY}")
    print(f"   Framework: {'Databricks DQX' if DQX_AVAILABLE else 'Basic Validation Fallback'}")
    print(f"   Threshold: {QUALITY_THRESHOLD:.1%}")
    print()
    print("üìä Usage Examples:")
    print(f"   -- Get all valid records:")
    print(f"   SELECT * FROM {SILVER_FULL_TABLE} WHERE flag_check = 'PASS'")
    print(f"   ")
    print(f"   -- Get all failed records with reasons:")
    print(f"   SELECT event_id, city, description_failure FROM {SILVER_FULL_TABLE} WHERE flag_check = 'FAIL'")
    print(f"   ")
    print(f"   -- Get quality metrics:")
    print(f"   SELECT flag_check, COUNT(*), AVG(dqx_quality_score) FROM {SILVER_FULL_TABLE} GROUP BY flag_check")

# Display final summary
display_enhanced_single_table_pipeline_summary()
print()
show_dqx_single_table_extension_summary()

<function_calls>
<invoke name="TodoWrite">
<parameter name="todos">[{"content": "Correct Bronze to Silver DQX pipeline to use single enhanced table instead of separate quarantine table", "status": "completed", "activeForm": "Correcting Bronze to Silver DQX pipeline to use single enhanced table instead of separate quarantine table"}]