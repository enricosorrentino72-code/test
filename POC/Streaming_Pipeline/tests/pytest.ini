# pytest configuration for Databricks Streaming Pipeline Tests
[tool:pytest]

# Test discovery
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*

# Minimum version
minversion = 6.0

# Add options
addopts = 
    -v
    --strict-markers
    --strict-config
    --tb=short
    --cov-report=html:htmlcov
    --cov-report=term-missing
    --cov-report=xml
    --cov-fail-under=75
    --junit-xml=test-results.xml

# Markers
markers =
    unit: Unit tests (fast, isolated)
    integration: Integration tests (slower, dependencies)
    performance: Performance tests
    critical: Critical priority functions (95% coverage)
    high: High priority functions (95% coverage)
    medium: Medium priority functions (80% coverage)
    low: Low priority functions (60% coverage)
    databricks: Tests requiring Databricks environment
    eventhub: Tests requiring EventHub connectivity
    dqx: Tests for DQX quality framework
    slow: Tests that take a long time to run
    cloud: Cloud integration tests (real Azure services)

# Coverage configuration
[coverage:run]
source = .
omit = 
    tests/*
    */test_*
    */__pycache__/*
    */venv/*
    */env/*
    setup.py

[coverage:report]
# Regexes for lines to exclude from consideration
exclude_lines =
    # Have to re-enable the standard pragma
    pragma: no cover

    # Don't complain about missing debug-only code:
    def __repr__
    if self\.debug

    # Don't complain if tests don't hit defensive assertion code:
    raise AssertionError
    raise NotImplementedError

    # Don't complain if non-runnable code isn't run:
    if 0:
    if __name__ == .__main__.:

    # Don't complain about abstract methods
    @(abc\.)?abstractmethod

# Coverage targets by priority
[coverage:html]
directory = htmlcov

[coverage:xml]
output = coverage.xml