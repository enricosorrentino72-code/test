# Pytest configuration file
# Enhanced configuration for comprehensive testing of the Purchase Order Item Streaming Pipeline

[pytest]
# Minimum version of pytest required
minversion = 7.0

# Test paths
testpaths = tests

# Python files pattern
python_files = test_*.py

# Python classes pattern
python_classes = Test*

# Python functions pattern
python_functions = test_*

# Command line options
addopts =
    # Verbosity and output options
    -v
    --strict-markers
    --tb=short
    --strict-config
    --showlocals

    # Coverage options
    --cov=class
    --cov=utility
    --cov-report=term-missing:skip-covered
    --cov-report=html:htmlcov
    --cov-report=xml:coverage.xml
    --cov-report=json:coverage.json
    --cov-fail-under=80
    --cov-branch

    # Performance options
    --benchmark-disable
    --durations=10
    --durations-min=0.1

    # Parallel execution
    -n auto
    --dist=loadscope

    # Output options
    --junit-xml=test-results.xml
    --html=pytest-report.html
    --self-contained-html

    # Warning options
    -W ignore::DeprecationWarning
    -W ignore::PendingDeprecationWarning

    # Doctest options
    --doctest-modules
    --doctest-continue-on-failure

# Test markers
markers =
    # Test categories
    unit: Unit tests for individual components
    integration: Integration tests for component interactions
    performance: Performance and benchmark tests
    cloud: Cloud-specific tests requiring Azure resources
    slow: Slow running tests (> 5 seconds)
    dqx: DQX framework specific tests

    # Test priorities
    critical: Critical tests that must pass
    high: High priority tests
    medium: Medium priority tests
    low: Low priority tests

    # Test requirements
    requires_eventhub: Tests requiring EventHub connection
    requires_spark: Tests requiring Spark session
    requires_databricks: Tests requiring Databricks runtime
    requires_adls: Tests requiring ADLS Gen2 access

    # Test characteristics
    smoke: Quick smoke tests for basic functionality
    regression: Regression tests for bug fixes
    security: Security-related tests
    data_quality: Data quality validation tests

# Filter warnings
filterwarnings =
    ignore::DeprecationWarning
    ignore::PendingDeprecationWarning
    ignore::ResourceWarning
    ignore::pytest.PytestUnraisableExceptionWarning
    error::UserWarning:purchase_order

# Console output options
console_output_style = progress

# Logging configuration
log_cli = true
log_cli_level = INFO
log_cli_format = %(asctime)s [%(levelname)8s] [%(name)s] %(message)s
log_cli_date_format = %Y-%m-%d %H:%M:%S

# File logging
log_file = tests/test.log
log_file_level = DEBUG
log_file_format = %(asctime)s [%(levelname)8s] [%(filename)s:%(lineno)d] %(message)s
log_file_date_format = %Y-%m-%d %H:%M:%S

# Capture options
capture = no

# Timeout for tests (in seconds)
timeout = 300
timeout_method = thread

# Test discovery options
python_classes = Test* *Tests *Test
python_functions = test_* *_test

# Doctest options
doctest_optionflags = NORMALIZE_WHITESPACE IGNORE_EXCEPTION_DETAIL

# Test ordering
# Randomize test order to detect hidden dependencies
# addopts = --random-order

# Xfail strict mode
xfail_strict = true

# Required plugins
required_plugins =
    pytest-cov>=4.1.0
    pytest-mock>=3.11.0
    pytest-benchmark>=4.0.0
    pytest-timeout>=2.1.0
    pytest-xdist>=3.3.0
    pytest-html>=3.2.0

# Environment variables for tests
env =
    PYTHONPATH = class:utility:tests
    ENVIRONMENT = test
    LOG_LEVEL = DEBUG

# Test collection options
collect_ignore = ["setup.py", "conftest.py"]
collect_ignore_glob = ["*_backup.py", "*.bak"]

# Assertion rewriting
enable_assertion_pass_hook = true

# Cache directory
cache_dir = .pytest_cache

# Maximum number of assertions to introspect
assertion_introspection = auto

# JUnit XML options
junit_suite_name = Purchase Order Item Pipeline Tests
junit_logging = all
junit_log_passing_tests = true
junit_duration_report = total

# Live logging
live_log_level = INFO
live_log_format = %(asctime)s [%(levelname)8s] %(message)s

# Benchmark options
benchmark_only = false
benchmark_skip = false
benchmark_disable = true
benchmark_max_time = 1.0
benchmark_min_time = 0.000005
benchmark_min_rounds = 5
benchmark_timer = time.perf_counter
benchmark_warmup = true
benchmark_warmup_iterations = 10000

# Code coverage contexts
cov_contexts =
    test_function
    test_class
    test_module

# Parallel execution settings
numprocesses = auto
maxprocesses = 4

# Flaky test handling
reruns = 2
reruns_delay = 1